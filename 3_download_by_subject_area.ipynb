{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE-ABS-KEY (\"Belief* Network\" OR \"Belief* System\" OR \"Belief* Map\" OR \"Belief* Set\" OR \"Issue* Network\" OR \"Issue* System\" OR \"Issue* Map\" OR \"Issue* Set\" OR \"Opinion* Network\" OR \"Opinion* System\" OR \"Opinion* Map\" OR \"Opinion* Set\" OR \"Value System\" OR \"Value Set\" OR \"Attitude* Network\" OR \"Attitude* System\" OR \"Attitude* Map\" OR \"Attitude* Set\" OR \"Cogni* System\" OR \"Cogni* Map\" OR \"Cogni* Set\" OR \"Network of Belief\" OR \"Network of Issue\" OR \"Network of Opinion\" OR \"Network of Attitudes\" OR \"Network of Cogni*\" OR \"System* of Beliefs\" OR \"System* of Issue\" OR \"System *of Opinion\" OR \"System* of Value\" OR \"System* of Attitude\" OR \"Map* of Belief\" OR \"Map* of Issue\" OR \"Map* of Opinion\" OR \"Map* of Attitude\" OR \"Relationship Between Belief\" OR \"Relationship Between Opinion\" OR \"Relationship Between Value\" OR \"Relationship Between Attitude\" OR \"Mental Map\")\n"
     ]
    }
   ],
   "source": [
    "keywords = [\n",
    "    # 'network',\n",
    "    'network of belief',\n",
    "    'networks of belief',\n",
    "    'belief network',\n",
    "    'beliefs network',\n",
    "    'map of belief',\n",
    "    'maps of belief',\n",
    "    'relationship between belief',\n",
    "    'cognitive map',\n",
    "    'map of cognitive behavior',\n",
    "    'map of opinion',\n",
    "    'network of opinion',\n",
    "    'networks of opinion',\n",
    "    'relationship between opinion',\n",
    "    'system of belief',\n",
    "    'systems of belief',\n",
    "    'belief system',\n",
    "    'beliefs system',\n",
    "    'set of opinion',\n",
    "    'opinion set',\n",
    "    'set of value',\n",
    "    'values set',\n",
    "    'system of opinion',\n",
    "    'opinion system', \n",
    "    'opimions system',\n",
    "    'system of value',\n",
    "    'values system',\n",
    "    'value system',\n",
    "    'attitude network',\n",
    "    'attitude map',\n",
    "    'relationship between attitude',\n",
    "    'attitude system',\n",
    "    # 'attitude',\n",
    "    'issue system'\n",
    "    'factor system',\n",
    "    'system of factor',\n",
    "    'variable system',\n",
    "    'system of variable',\n",
    "    'crucial factor',\n",
    "    # additional ones after 2023.11.20\n",
    "    'correlational class analysis',\n",
    "    'relational class analysis',\n",
    "    'ideology system',\n",
    "    'map of idea',\n",
    "    'system of idea',\n",
    "    # use things like * in scopus: \"behav* network\" would match behavior network and behavioural network both; from https://library.bath.ac.uk/scopus/keyword-searching\n",
    "]\n",
    "\n",
    "\n",
    "systematic_keywords = [\n",
    "    'Belief* Network',\n",
    "    'Belief* System',\n",
    "    'Belief* Map',\n",
    "    'Belief* Set',\n",
    "    'Issue* Network',\n",
    "    'Issue* System',\n",
    "    'Issue* Map',\n",
    "    'Issue* Set',\n",
    "    'Opinion* Network',\n",
    "    'Opinion* System', # keep an eye on this one\n",
    "    'Opinion* Map',\n",
    "    'Opinion* Set',\n",
    "    # 'Value Network', catches valued networks too, physics heavy application\n",
    "    # 'Values Network',\n",
    "    'Value System',\n",
    "    # 'Value* Map', business heavy application\n",
    "    'Value Set',\n",
    "    'Attitude* Network',\n",
    "    'Attitude* System',\n",
    "    'Attitude* Map',\n",
    "    'Attitude* Set',\n",
    "    # 'Cogni* Network', catches engineering and medical applications\n",
    "    'Cogni* System',\n",
    "    'Cogni* Map', # cognitive catches a lot of things, FUZZY exclude needed\n",
    "    'Cogni* Set', \n",
    "    'Network of Belief',\n",
    "    'Network of Issue',\n",
    "    'Network of Opinion',\n",
    "    # 'Network of Values', catching SBM stuff\n",
    "    'Network of Attitudes',\n",
    "    'Network of Cogni*', # FINISHED HERE 2023.11.20\n",
    "    'System* of Beliefs',\n",
    "    'System* of Issue',\n",
    "    'System *of Opinion',\n",
    "    'System* of Value',\n",
    "    'System* of Attitude',\n",
    "    # 'System* of Cogni*', catches neuroscience and dual process stuff\n",
    "    'Map* of Belief',\n",
    "    'Map* of Issue',\n",
    "    'Map* of Opinion',\n",
    "    # 'Map of Values',\n",
    "    'Map* of Attitude',\n",
    "    # 'Map of Cognitives', neuroscience heavy application\n",
    "    # 'Set of Beliefs',\n",
    "    # 'Set of Issues',\n",
    "    # 'Set of Opinions',\n",
    "    # 'Set of Values',\n",
    "    # 'Set of Attitudes',\n",
    "    # 'Set of Cognitives', lot of noise in all the \"set\" keywords\n",
    "    'Relationship Between Belief',\n",
    "    # 'Relationship Between of Issue' , ?\n",
    "    'Relationship Between Opinion',\n",
    "    'Relationship Between Value',\n",
    "    'Relationship Between Attitude',\n",
    "    # 'Relationship Between of Cognitives',\n",
    "    \"Mental Map\",\n",
    "]\n",
    "\n",
    "\n",
    "joker_keywords = [\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "def generate_query(keywords):\n",
    "    # Joining keywords with ' OR ' and enclosing each in quotation marks\n",
    "    joined_keywords = ' OR '.join(f'\"{keyword}\"' for keyword in keywords)\n",
    "    # Constructing the final query\n",
    "    query = f'TITLE-ABS-KEY ({joined_keywords})'\n",
    "    return query\n",
    "\n",
    "query = generate_query(systematic_keywords)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique papers\n",
      "by title:  40729\n",
      "by DOI:  36169\n",
      "by EID:  40938\n"
     ]
    }
   ],
   "source": [
    "# Mapping of file numbers to categories\n",
    "categories = {\n",
    "    # 1: \"Computer Science Pre 2017\",\n",
    "    1: \"Computer Science\",\n",
    "    # 2: \"Computer Science Post 2017\",\n",
    "    2: \"Computer Science\",\n",
    "    # 3: \"Engineering Pre 2017\",\n",
    "    3: \"Engineering\",\n",
    "    # 4: \"Engineering Post 2017\",\n",
    "    4: \"Engineering\",\n",
    "    5: \"Social Sciences\",\n",
    "    6: \"Mathematics\",\n",
    "    7: \"Psychology\",\n",
    "    8: \"Arts and Humanities\",\n",
    "    9: \"Medicine\",\n",
    "    10: \"Neuroscience\",\n",
    "    11: \"Business Management and Accounting\",\n",
    "    12: \"Environmental Science\",\n",
    "    13: \"Decision Sciences\",\n",
    "    14: \"Physics and Astronomy\",\n",
    "    15: \"Agricultural and Biological Sciences\",\n",
    "    16: \"Materials Science\",\n",
    "    17: \"Economics Econometrics Finance\",\n",
    "    18: \"Biochemistry Genetics and Molecular Biology\",\n",
    "    19: \"Earth and Planetary Sciences\",\n",
    "    20: \"Energy\",\n",
    "    21: \"Health Professions\",\n",
    "    22: \"Nursing\",\n",
    "    23: \"Multidisciplinary\",\n",
    "    24: \"Chemical Engineering\",\n",
    "    25: \"Chemistry\",\n",
    "    26: \"Pharmacology Toxicology and Pharmaceutics\",\n",
    "    27: \"Immunology and Microbiology\",\n",
    "    28: \"Veterinary\",\n",
    "    29: \"Dentistry\",\n",
    "    30: \"Undefined\",\n",
    "}\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"data/by category/raw/\"  # Replace with the path to your files\n",
    "\n",
    "# List to hold all dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"scopus\") and filename.endswith(\".csv\"):\n",
    "        # Extract the number from the filename\n",
    "        file_number = int(filename.split('(')[1].split(')')[0])-1\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        # Add the category column\n",
    "        df['Category'] = categories.get(file_number, \"Unknown\")\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Now dataframes is a list of dataframes with the category column added\n",
    "# concat all dataframes into one\n",
    "papers = pd.concat(dataframes)\n",
    "\n",
    "# number of unique papers \n",
    "print(\"number of unique papers\")\n",
    "print(\"by title: \", len(papers['Title'].unique()))\n",
    "print(\"by DOI: \", len(papers['DOI'].unique()))\n",
    "print(\"by EID: \", len(papers['EID'].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by a unique identifier (e.g., 'EID') and aggregate categories\n",
    "unified_papers = papers.groupby('EID').agg({\n",
    "    'Authors': 'first', \n",
    "    'Author full names': 'first',\n",
    "    'Author(s) ID': 'first',\n",
    "    'Title': 'first',\n",
    "    'Year': 'first',\n",
    "    'Source title': 'first',\n",
    "    'Volume': 'first',\n",
    "    'Issue': 'first',\n",
    "    'Art. No.': 'first',\n",
    "    'Page start': 'first',\n",
    "    'Page end': 'first',\n",
    "    'Page count': 'first',\n",
    "    'Cited by': 'first',\n",
    "    'DOI': 'first',\n",
    "    'Link': 'first',\n",
    "    'Abstract': 'first',\n",
    "    'Author Keywords': 'first',\n",
    "    'Index Keywords': 'first',\n",
    "    'References': 'first',\n",
    "    'Document Type': 'first',\n",
    "    'Publication Stage': 'first',\n",
    "    'Open Access': 'first',\n",
    "    'Source': 'first',\n",
    "    # Category column: aggregate into a string of unique categories\n",
    "    'Category': lambda x: list(set(x))\n",
    "}).reset_index()\n",
    "\n",
    "unified_papers.to_pickle('data/by category/unified.pkl')\n",
    "\n",
    "\n",
    "unified_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scopus_scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
